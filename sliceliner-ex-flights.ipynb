{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608034f5-31f7-4910-9144-5d153934c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Jan 17 07:09:18 PM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Jan 17 07:09:18 PM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (pipeline.py, line 57)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[1], line 9\u001b[0m\n    from dt import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Code/dt-demo-py/dt/__init__.py:2\u001b[0;36m\n\u001b[0;31m    from .pipeline import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Code/dt-demo-py/dt/pipeline.py:57\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"slice_losses\": None # TODO: Figure out how to implement this\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import statistics as sts\n",
    "from sliceline.slicefinder import Slicefinder\n",
    "import optbinning\n",
    "from dt import *\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef4d1b-fc71-430b-ae1f-4b124559335b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We'll be testing out the Department of Transportation Airline On-Time Statistics dataset. We will set data sources to be years, which are 2018 to 2023. (2023 doesn't have November and December available yet.) The training columns will be:\n",
    "\n",
    "1. Note: Year is excluded from training (by stationarity assumption)\n",
    "2. Month (int)\n",
    "3. Day (int)\n",
    "4. Day of week (int)\n",
    "5. Marketing airline (categorical, total 9): This is the airline which sold the ticket. \n",
    "6. Operating airline (categorical, total 21): This is the airline which operates the airplane. Could be the same as marketing airline, could be a different regional operator.\n",
    "7. Origin & destination states (categorical, total 51)\n",
    "8. Origin & destination airport coordinates (continuous): The original data is categorical with high number of options, it is probably more meaningful & tractable to extract the coordinates.\n",
    "9. Distance (in miles)\n",
    "10. Scheduled arrival & departure time (integer from 0000 to 2400). \n",
    "\n",
    "The prediction column is arrival delay (in minutes) which is encoded as integer, where negative is early and positive is late arrival. \n",
    "\n",
    "This dataset is very large: There are almost 40 million rows in total. \n",
    "\n",
    "This dataset is also very high-dimensional: there are 10 ordinal features and 132 categorical one-hot encoded features (30 airlines + 102 states). It is a good stress test for the system. \n",
    "\n",
    "\n",
    " - NOTE: 2018-08 CSV is malformed and gives a parse error; needs fixing\n",
    " - NOTE: There may be issues where a flight has no arrival time due to cancellation, with unexpected handling of the arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7d7d4-de60-4493-8b01-fbb9a98ca8a1",
   "metadata": {},
   "source": [
    "**Reproducibility Notes**\n",
    "\n",
    "The PyPi `sliceline` package requires Python 3.7~3.10.0, which is not the most up-to-date python version. Creating a virtual environment with python 3.9 should work. The conda environment should also have:\n",
    "\n",
    "* sliceline\n",
    "* pandas\n",
    "* scikit\n",
    "* optbinning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcd032-9b0c-4bad-9588-3ce4f2883e98",
   "metadata": {},
   "source": [
    "## Defining Reusable Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb3e8d-90b7-4a52-8904-71431ceaec38",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62a1fc-5cee-4082-a442-e44c4b6a48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data_psql(db_source):\n",
    "    df = db_source.get_query_result()\n",
    "    train_x, train_y = dt.split_xy(df, db_source.y)\n",
    "    return train_x, train_y, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d8ab3-3642-472c-8eb9-63635045138c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61585a-010e-4da2-9792-176a6565f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y):\n",
    "    model = HistGradientBoostingRegressor(random_state=42)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d634a1-d3ab-466c-8cf8-b9d5479dfa10",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c907bf-ae7c-4a0d-ab24-aabdb6e539e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(model, x, y):\n",
    "    preds = model.predict(x)\n",
    "    training_errors = (y - preds)**2\n",
    "    return training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392f377-a6b3-4e8d-9c13-9c20e20d7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(arr):\n",
    "    means = sts.mean(arr)\n",
    "    rms = math.sqrt(means)\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bcfc3-8c71-4326-b642-7bcd6157a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_error(model, x, y):\n",
    "    errors = get_errors(model, x, y)\n",
    "    return get_rms(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ed445-9ef4-45ba-82c1-06f7628b5330",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cccd4-c960-45f2-bb45-e56b84385238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_xs(train_x, train_errors):\n",
    "    optimal_binner = optbinning.ContinuousOptimalBinning(max_n_prebins=20, max_n_bins=20)\n",
    "    train_x_binned = pd.DataFrame(np.array(\n",
    "        [\n",
    "            optimal_binner.fit_transform(train_x[col], train_errors, metric=\"bins\") for col in train_x.columns\n",
    "        ]\n",
    "    ).T, columns=train_x.columns)\n",
    "    return train_x_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c46518-dffb-48c5-9555-755b5e5ea858",
   "metadata": {},
   "source": [
    "### Sliceliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bfca7-f913-4575-9ca2-634ca2630042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(train_x_binned, train_errors, alpha = 0.9, k=1, max_l = 3, min_sup = 0, verbose = False):\n",
    "    sf = Slicefinder(alpha = alpha, k = k, max_l = max_l, min_sup = min_sup, verbose = verbose)\n",
    "    sf.fit(train_x_binned, train_errors)\n",
    "    print(sf)\n",
    "    df = pd.DataFrame(sf.top_slices_, columns=sf.feature_names_in_, index=sf.get_feature_names_out())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f1f5f-4583-4ec8-980c-6e552e1242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat slices returned from sliceliner as dataframe into a matrix of strings\n",
    "def reformat_slices(slice_df):\n",
    "    slice_df.fillna('(-inf, inf)', inplace=True)\n",
    "    slice_list = slice_df.values.tolist()\n",
    "    slice_parsed = dt.parse_slices(slice_list)\n",
    "    return slice_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2303fd8-af99-44c6-a5c5-d4db490663b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of times each slice already exists in xs\n",
    "def get_slice_cnts(xs, slices):\n",
    "    cnts = []\n",
    "    for slice_ in slices:\n",
    "        cnt = 0\n",
    "        for x in xs.values.tolist():\n",
    "            if dt.belongs_to_slice(slice_, x):\n",
    "                cnt += 1 \n",
    "        cnts.append(cnt)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc02db-3764-4495-b267-5b53d8de20bf",
   "metadata": {},
   "source": [
    "### Putting the Pipeline Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd15bc5-736f-4900-acaf-b07db9188117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model, report errors, and return model & binned train set\n",
    "def pipeline_train(train_x, train_y, test_x, test_y):\n",
    "    # Train model\n",
    "    model = train_model(train_x, train_y)\n",
    "    # Error analysis\n",
    "    train_errors = get_errors(model, train_x, train_y)\n",
    "    print(\"Train RMS error:\", get_rms(train_errors))\n",
    "    print(\"Test RMS error:\", get_rms(get_errors(model, test_x, test_y)))\n",
    "    # Binning\n",
    "    train_x_binned = bin_xs(train_x, train_errors)\n",
    "    return model, train_x_binned, train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a62c0-0bb3-4130-a839-77e88f7ffe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_sliceline(train_x, train_x_binned, train_errors, alpha = 0.9, max_l = 3, min_sup = 0, k = 1):\n",
    "    # Sliceliner\n",
    "    slices_df = get_slices(train_x_binned, train_errors, alpha = 0.9, max_l = 3, min_sup = 0, verbose = False, k=k)\n",
    "    slices = reformat_slices(slices_df)\n",
    "    existing_cnts = get_slice_cnts(train_x, slices)\n",
    "    print(\"Slices:\")\n",
    "    print(slices_df)\n",
    "    print(\"Existing counts:\", existing_cnts)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d2d4b-ca27-4950-8d80-61dfdb546ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain additional data\n",
    "def pipeline_dt(sources, costs, slices, query_counts):\n",
    "    dt = DT(sources, costs, slices, None)\n",
    "    additional_data = dt.run(query_counts)\n",
    "    return additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece0af3-f67a-4769-8a13-a5b56ca0e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine existing dataset with additional data\n",
    "# Additional data is shuffled in\n",
    "def pipeline_augment(train_x, train_y, additional_data, features):\n",
    "    add_df = pd.DataFrame(additional_data, columns=features)\n",
    "    add_x, add_y = dt.split_xy(add_df, 'arrival_delay')\n",
    "    aug_x = pd.concat([train_x, add_x], ignore_index=True)\n",
    "    aug_x = aug_x.sample(frac=1, random_state=12345)\n",
    "    aug_y = pd.concat([train_y, add_y], ignore_index=True)\n",
    "    aug_y = aug_y.sample(frac=1, random_state=12345)\n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dee5eb-348a-4f7c-81e8-e72573c3d388",
   "metadata": {},
   "source": [
    "## Flights Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a28ed-a5a6-4e5c-8e4c-1a3bc4e72a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_query_str = \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL ORDER BY random() LIMIT 10000;\"\n",
    "\n",
    "orig_dbsource = DBSource('localhost','dtdemo','jwc','postgres',train_test_query_str,'arrival_delay')\n",
    "\n",
    "# Yes, we will use simple uniform random subsampling even though it's bad practice for this proof of concept\n",
    "# The dataset is very large so we can get away with it\n",
    "# We also filter only rows that are not cancelled throughout this proof of concept\n",
    "train_x, train_y, train = grab_data_psql(orig_dbsource)\n",
    "train_x = dt.process_df(train_x)\n",
    "test_x, test_y, test = grab_data_psql(orig_dbsource)\n",
    "test_x = dt.process_df(test_x)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce7b40-2905-4e75-afa4-56afc23d9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x.columns))\n",
    "print(len(test_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a68e7-da88-422b-a1bf-a22b98433604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_x_binned, train_errors = pipeline_train(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e5a425-416f-4239-b262-c7728f692d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_sliceline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m slices \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_sliceline\u001b[49m(train_x, train_x_binned, train_errors, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, max_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, min_sup \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline_sliceline' is not defined"
     ]
    }
   ],
   "source": [
    "slices = pipeline_sliceline(train_x, train_x_binned, train_errors, alpha = 0.5, max_l = 1, min_sup = 0, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86876618-cd46-4096-b801-b18b5a880704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for defining data sources\n",
    "# Data sources are divided by year\n",
    "source_query_strs = [\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2018 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2019 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2020 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2021 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2022 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2023 ORDER BY random() LIMIT 10000;\",\n",
    "]\n",
    "sources = [dt.DBSource('localhost','dtdemo','jwc','postgres',query,'arrival_delay') for query in source_query_strs]\n",
    "costs = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11cac8c-64f7-4e4b-8de6-947219649f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary, for now\n",
    "query_counts = [ 100, 100, 100, 100, 100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131dbc40-d03a-48a1-823d-363d35e2bcae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DT' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m additional_data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_dt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m additional_data\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mpipeline_dt\u001b[0;34m(sources, costs, slices, query_counts)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline_dt\u001b[39m(sources, costs, slices, query_counts):\n\u001b[1;32m      3\u001b[0m     dt \u001b[38;5;241m=\u001b[39m DT(sources, costs, slices, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     additional_data \u001b[38;5;241m=\u001b[39m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m additional_data\n",
      "File \u001b[0;32m~/Code/dt-demo-py/dt/dt.py:85\u001b[0m, in \u001b[0;36mDT.run\u001b[0;34m(self, query_counts)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratiocoll(query_counts)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexploreexploit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_counts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dt-demo-py/dt/dt.py:93\u001b[0m, in \u001b[0;36mDT.exploreexploit\u001b[0;34m(self, query_counts)\u001b[0m\n\u001b[1;32m     90\u001b[0m unified_ys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m remaining_query \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(query_counts)\n\u001b[0;32m---> 93\u001b[0m explore_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m Q \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     95\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(remaining_query \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DT' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "additional_data = pipeline_dt(sources, costs, slices, query_counts)\n",
    "additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224e7ea-6667-4322-97f8-fcd8a4e94150",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_x, aug_y = pipeline_augment(train_x, train_y, additional_data, train.columns)\n",
    "aug_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf5ee4-708f-45c5-8caa-80e29a8a6bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
