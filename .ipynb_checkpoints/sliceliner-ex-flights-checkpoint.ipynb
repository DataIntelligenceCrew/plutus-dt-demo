{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608034f5-31f7-4910-9144-5d153934c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Jan 16 11:05:29 AM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Jan 16 11:05:29 AM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import statistics as sts\n",
    "from sliceline.slicefinder import Slicefinder\n",
    "import optbinning\n",
    "from dt import *\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef4d1b-fc71-430b-ae1f-4b124559335b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We'll be testing out the Department of Transportation Airline On-Time Statistics dataset. We will set data sources to be years, which are 2018 to 2023. (2023 doesn't have November and December available yet.) The training columns will be:\n",
    "\n",
    "1. Note: Year is excluded from training (by stationarity assumption)\n",
    "2. Month (int)\n",
    "3. Day (int)\n",
    "4. Day of week (int)\n",
    "5. Marketing airline (categorical, total 9): This is the airline which sold the ticket. \n",
    "6. Operating airline (categorical, total 21): This is the airline which operates the airplane. Could be the same as marketing airline, could be a different regional operator.\n",
    "7. Origin & destination states (categorical, total 51)\n",
    "8. Origin & destination airport coordinates (continuous): The original data is categorical with high number of options, it is probably more meaningful & tractable to extract the coordinates.\n",
    "9. Distance (in miles)\n",
    "10. Scheduled arrival & departure time (integer from 0000 to 2400). \n",
    "\n",
    "The prediction column is arrival delay (in minutes) which is encoded as integer, where negative is early and positive is late arrival. \n",
    "\n",
    "This dataset is very large: There are almost 40 million rows in total. \n",
    "\n",
    "This dataset is also very high-dimensional: there are 10 ordinal features and 132 categorical one-hot encoded features (30 airlines + 102 states). It is a good stress test for the system. \n",
    "\n",
    "\n",
    " - NOTE: 2018-08 CSV is malformed and gives a parse error; needs fixing\n",
    " - NOTE: There may be issues where a flight has no arrival time due to cancellation, with unexpected handling of the arrival_delay column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7d7d4-de60-4493-8b01-fbb9a98ca8a1",
   "metadata": {},
   "source": [
    "**Reproducibility Notes**\n",
    "\n",
    "The PyPi `sliceline` package requires Python 3.7~3.10.0, which is not the most up-to-date python version. Creating a virtual environment with python 3.9 should work. The conda environment should also have:\n",
    "\n",
    "* sliceline\n",
    "* pandas\n",
    "* scikit\n",
    "* optbinning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcd032-9b0c-4bad-9588-3ce4f2883e98",
   "metadata": {},
   "source": [
    "## Defining Reusable Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb3e8d-90b7-4a52-8904-71431ceaec38",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be62a1fc-5cee-4082-a442-e44c4b6a48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data_psql(db_source):\n",
    "    df = db_source.get_query_result()\n",
    "    train_x, train_y = dt.split_xy(df, db_source.y)\n",
    "    return train_x, train_y, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d8ab3-3642-472c-8eb9-63635045138c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a61585a-010e-4da2-9792-176a6565f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y):\n",
    "    model = HistGradientBoostingRegressor(random_state=42)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d634a1-d3ab-466c-8cf8-b9d5479dfa10",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c907bf-ae7c-4a0d-ab24-aabdb6e539e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(model, x, y):\n",
    "    preds = model.predict(x)\n",
    "    training_errors = (y - preds)**2\n",
    "    return training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4392f377-a6b3-4e8d-9c13-9c20e20d7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(arr):\n",
    "    means = sts.mean(arr)\n",
    "    rms = math.sqrt(means)\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5bcfc3-8c71-4326-b642-7bcd6157a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms_error(model, x, y):\n",
    "    errors = get_errors(model, x, y)\n",
    "    return get_rms(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ed445-9ef4-45ba-82c1-06f7628b5330",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9cccd4-c960-45f2-bb45-e56b84385238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_xs(train_x, train_errors):\n",
    "    optimal_binner = optbinning.ContinuousOptimalBinning(max_n_prebins=20, max_n_bins=20)\n",
    "    train_x_binned = pd.DataFrame(np.array(\n",
    "        [\n",
    "            optimal_binner.fit_transform(train_x[col], train_errors, metric=\"bins\") for col in train_x.columns\n",
    "        ]\n",
    "    ).T, columns=train_x.columns)\n",
    "    return train_x_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c46518-dffb-48c5-9555-755b5e5ea858",
   "metadata": {},
   "source": [
    "### Sliceliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1bfca7-f913-4575-9ca2-634ca2630042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(train_x_binned, train_errors, alpha = 0.9, k=1, max_l = 3, min_sup = 0, verbose = False):\n",
    "    sf = Slicefinder(alpha = alpha, k = k, max_l = max_l, min_sup = min_sup, verbose = verbose)\n",
    "    sf.fit(train_x_binned, train_errors)\n",
    "    df = pd.DataFrame(sf.top_slices_, columns=sf.feature_names_in_, index=sf.get_feature_names_out())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009f1f5f-4583-4ec8-980c-6e552e1242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat slices returned from sliceliner as dataframe into a matrix of strings\n",
    "def reformat_slices(slice_df):\n",
    "    slice_df.fillna('(-inf, inf)', inplace=True)\n",
    "    slice_list = slice_df.values.tolist()\n",
    "    slice_parsed = dt.parse_slices(slice_list)\n",
    "    return slice_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2303fd8-af99-44c6-a5c5-d4db490663b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of times each slice already exists in xs\n",
    "def get_slice_cnts(xs, slices):\n",
    "    cnts = []\n",
    "    for slice_ in slices:\n",
    "        cnt = 0\n",
    "        for x in xs.values.tolist():\n",
    "            if dt.belongs_to_slice(slice_, x):\n",
    "                cnt += 1 \n",
    "        cnts.append(cnt)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc02db-3764-4495-b267-5b53d8de20bf",
   "metadata": {},
   "source": [
    "### Putting the Pipeline Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd15bc5-736f-4900-acaf-b07db9188117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model, report errors, and return model & binned train set\n",
    "def pipeline_train(train_x, train_y, test_x, test_y):\n",
    "    # Train model\n",
    "    model = train_model(train_x, train_y)\n",
    "    # Error analysis\n",
    "    train_errors = get_errors(model, train_x, train_y)\n",
    "    print(\"Train RMS error:\", get_rms(train_errors))\n",
    "    print(\"Test RMS error:\", get_rms(get_errors(model, test_x, test_y)))\n",
    "    # Binning\n",
    "    train_x_binned = bin_xs(train_x, train_errors)\n",
    "    return model, train_x_binned, train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602a62c0-0bb3-4130-a839-77e88f7ffe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_sliceline(train_x, train_x_binned, train_errors, alpha = 0.9, max_l = 3, min_sup = 0, k = 1):\n",
    "    # Sliceliner\n",
    "    slices_df = get_slices(train_x_binned, train_errors, alpha = 0.9, max_l = 3, min_sup = 0, verbose = False, k=k)\n",
    "    slices = reformat_slices(slices_df)\n",
    "    existing_cnts = get_slice_cnts(train_x, slices)\n",
    "    print(\"Slices:\")\n",
    "    print(slices_df)\n",
    "    print(\"Existing counts:\", existing_cnts)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "412d2d4b-ca27-4950-8d80-61dfdb546ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain additional data\n",
    "def pipeline_dt(sources, costs, slices, query_counts):\n",
    "    dt = DT(sources, costs, slices, None)\n",
    "    additional_data = dt.run(query_counts)\n",
    "    return additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ece0af3-f67a-4769-8a13-a5b56ca0e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine existing dataset with additional data\n",
    "# Additional data is shuffled in\n",
    "def pipeline_augment(train_x, train_y, additional_data, features):\n",
    "    add_df = pd.DataFrame(additional_data, columns=features)\n",
    "    add_x, add_y = dt.split_xy(add_df, 'arrival_delay')\n",
    "    aug_x = pd.concat([train_x, add_x], ignore_index=True)\n",
    "    aug_x = aug_x.sample(frac=1, random_state=12345)\n",
    "    aug_y = pd.concat([train_y, add_y], ignore_index=True)\n",
    "    aug_y = aug_y.sample(frac=1, random_state=12345)\n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dee5eb-348a-4f7c-81e8-e72573c3d388",
   "metadata": {},
   "source": [
    "## Flights Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188a28ed-a5a6-4e5c-8e4c-1a3bc4e72a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>origin_longitude</th>\n",
       "      <th>origin_latitude</th>\n",
       "      <th>dest_longitude</th>\n",
       "      <th>dest_latitude</th>\n",
       "      <th>distance</th>\n",
       "      <th>departure_scheduled</th>\n",
       "      <th>arrival_scheduled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>-97.029722</td>\n",
       "      <td>32.894444</td>\n",
       "      <td>-106.377222</td>\n",
       "      <td>31.806667</td>\n",
       "      <td>551</td>\n",
       "      <td>1125</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>-84.428056</td>\n",
       "      <td>33.636667</td>\n",
       "      <td>-86.391111</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>147</td>\n",
       "      <td>900</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>-82.535278</td>\n",
       "      <td>27.973056</td>\n",
       "      <td>-95.340000</td>\n",
       "      <td>29.983333</td>\n",
       "      <td>787</td>\n",
       "      <td>937</td>\n",
       "      <td>1108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>-96.850833</td>\n",
       "      <td>32.845833</td>\n",
       "      <td>-118.408056</td>\n",
       "      <td>33.942500</td>\n",
       "      <td>1246</td>\n",
       "      <td>2045</td>\n",
       "      <td>2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-97.029722</td>\n",
       "      <td>32.894444</td>\n",
       "      <td>-106.377222</td>\n",
       "      <td>31.806667</td>\n",
       "      <td>551</td>\n",
       "      <td>1338</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>-84.428056</td>\n",
       "      <td>33.636667</td>\n",
       "      <td>-73.872500</td>\n",
       "      <td>40.777222</td>\n",
       "      <td>762</td>\n",
       "      <td>1930</td>\n",
       "      <td>2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>-82.535278</td>\n",
       "      <td>27.973056</td>\n",
       "      <td>-104.879722</td>\n",
       "      <td>39.774444</td>\n",
       "      <td>1506</td>\n",
       "      <td>1835</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.872500</td>\n",
       "      <td>40.777222</td>\n",
       "      <td>-84.668611</td>\n",
       "      <td>39.047778</td>\n",
       "      <td>585</td>\n",
       "      <td>1015</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>-122.375556</td>\n",
       "      <td>37.618889</td>\n",
       "      <td>-71.006389</td>\n",
       "      <td>42.363056</td>\n",
       "      <td>2704</td>\n",
       "      <td>2155</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-115.158889</td>\n",
       "      <td>36.079722</td>\n",
       "      <td>-117.187222</td>\n",
       "      <td>32.732778</td>\n",
       "      <td>258</td>\n",
       "      <td>545</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day  weekday  origin_longitude  origin_latitude  \\\n",
       "0     2022      9   22        4        -97.029722        32.894444   \n",
       "1     2021      2   19        5        -84.428056        33.636667   \n",
       "2     2019     12   24        2        -82.535278        27.973056   \n",
       "3     2020     11   24        2        -96.850833        32.845833   \n",
       "4     2022      1    4        2        -97.029722        32.894444   \n",
       "...    ...    ...  ...      ...               ...              ...   \n",
       "9995  2018      2    8        4        -84.428056        33.636667   \n",
       "9996  2023      1   24        2        -82.535278        27.973056   \n",
       "9997  2022      8   29        1        -73.872500        40.777222   \n",
       "9998  2023      8   14        1       -122.375556        37.618889   \n",
       "9999  2019      9    9        1       -115.158889        36.079722   \n",
       "\n",
       "      dest_longitude  dest_latitude  distance  departure_scheduled  \\\n",
       "0        -106.377222      31.806667       551                 1125   \n",
       "1         -86.391111      32.300000       147                  900   \n",
       "2         -95.340000      29.983333       787                  937   \n",
       "3        -118.408056      33.942500      1246                 2045   \n",
       "4        -106.377222      31.806667       551                 1338   \n",
       "...              ...            ...       ...                  ...   \n",
       "9995      -73.872500      40.777222       762                 1930   \n",
       "9996     -104.879722      39.774444      1506                 1835   \n",
       "9997      -84.668611      39.047778       585                 1015   \n",
       "9998      -71.006389      42.363056      2704                 2155   \n",
       "9999     -117.187222      32.732778       258                  545   \n",
       "\n",
       "      arrival_scheduled  \n",
       "0                  1203  \n",
       "1                   859  \n",
       "2                  1108  \n",
       "3                  2215  \n",
       "4                  1427  \n",
       "...                 ...  \n",
       "9995               2153  \n",
       "9996               2040  \n",
       "9997               1240  \n",
       "9998                632  \n",
       "9999                655  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_query_str = \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL ORDER BY random() LIMIT 10000;\"\n",
    "\n",
    "orig_dbsource = DBSource('localhost','dtdemo','jwc','postgres',train_test_query_str,'arrival_delay')\n",
    "\n",
    "# Yes, we will use simple uniform random subsampling even though it's bad practice for this proof of concept\n",
    "# The dataset is very large so we can get away with it\n",
    "# We also filter only rows that are not cancelled throughout this proof of concept\n",
    "train_x, train_y, train = grab_data_psql(orig_dbsource)\n",
    "train_x = dt.process_df(train_x)\n",
    "test_x, test_y, test = grab_data_psql(orig_dbsource)\n",
    "test_x = dt.process_df(test_x)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dce7b40-2905-4e75-afa4-56afc23d9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x.columns))\n",
    "print(len(test_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1a68e7-da88-422b-a1bf-a22b98433604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMS error: 36.486052386369224\n",
      "Test RMS error: 49.15516962384113\n"
     ]
    }
   ],
   "source": [
    "model, train_x_binned, train_errors = pipeline_train(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e5a425-416f-4239-b262-c7728f692d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/validation.py:457: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  and array.dtypes.apply(is_sparse).any()\n",
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/validation.py:294: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/validation.py:303: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/slicefinder.py:329: RuntimeWarning: divide by zero encountered in divide\n",
      "  (\n",
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/slicefinder.py:365: RuntimeWarning: invalid value encountered in divide\n",
      "  (slice_errors / slice_sizes) / self.average_error_ - 1\n",
      "/home/jwc/miniconda3/envs/sliceline-venv/lib/python3.9/site-packages/sliceline/slicefinder.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  ) - (1 - self.alpha) * (n_row_x_encoded / slice_sizes - 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slices:\n",
      "                   year        month          day      weekday  \\\n",
      "slice_0  [2020.50, inf)  (-inf, inf)  (-inf, inf)  [2.50, inf)   \n",
      "slice_1  [2020.50, inf)  (-inf, inf)  (-inf, inf)  [2.50, inf)   \n",
      "slice_2  [2020.50, inf)  (-inf, inf)  (-inf, inf)  (-inf, inf)   \n",
      "slice_3  [2020.50, inf)  (-inf, inf)  (-inf, inf)  (-inf, inf)   \n",
      "slice_4  [2020.50, inf)  (-inf, inf)  (-inf, inf)  (-inf, inf)   \n",
      "\n",
      "        origin_longitude origin_latitude     dest_longitude dest_latitude  \\\n",
      "slice_0      (-inf, inf)     (-inf, inf)        (-inf, inf)  [34.68, inf)   \n",
      "slice_1   [-106.74, inf)     (-inf, inf)        (-inf, inf)   (-inf, inf)   \n",
      "slice_2   [-106.74, inf)     (-inf, inf)        (-inf, inf)  [34.68, inf)   \n",
      "slice_3      (-inf, inf)    [39.30, inf)        (-inf, inf)  [34.68, inf)   \n",
      "slice_4      (-inf, inf)    [39.30, inf)  [-106.96, -74.37)   (-inf, inf)   \n",
      "\n",
      "            distance departure_scheduled arrival_scheduled  \n",
      "slice_0  (-inf, inf)         (-inf, inf)       (-inf, inf)  \n",
      "slice_1  (-inf, inf)         (-inf, inf)       (-inf, inf)  \n",
      "slice_2  (-inf, inf)         (-inf, inf)       (-inf, inf)  \n",
      "slice_3  (-inf, inf)         (-inf, inf)       (-inf, inf)  \n",
      "slice_4  (-inf, inf)         (-inf, inf)       (-inf, inf)  \n",
      "Existing counts: [2114, 2529, 2215, 1299, 1210]\n"
     ]
    }
   ],
   "source": [
    "slices = pipeline_sliceline(train_x, train_x_binned, train_errors, alpha = 0.5, max_l = 1, min_sup = 0, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86876618-cd46-4096-b801-b18b5a880704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for defining data sources\n",
    "# Data sources are divided by year\n",
    "source_query_strs = [\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2018 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2019 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2020 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2021 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2022 ORDER BY random() LIMIT 10000;\",\n",
    "    \"SELECT year,month,day,weekday,origin_longitude,origin_latitude,dest_longitude,dest_latitude,distance,departure_scheduled,arrival_scheduled,arrival_delay FROM flights WHERE arrival_delay IS NOT NULL AND year = 2023 ORDER BY random() LIMIT 10000;\",\n",
    "]\n",
    "sources = [dt.DBSource('localhost','dtdemo','jwc','postgres',query,'arrival_delay') for query in source_query_strs]\n",
    "costs = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b11cac8c-64f7-4e4b-8de6-947219649f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary, for now\n",
    "query_counts = [ 100, 100, 100, 100, 100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "131dbc40-d03a-48a1-823d-363d35e2bcae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m additional_data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_dt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m additional_data\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mpipeline_dt\u001b[0;34m(sources, costs, slices, query_counts)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline_dt\u001b[39m(sources, costs, slices, query_counts):\n\u001b[0;32m----> 3\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[43mDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     additional_data \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mrun(query_counts)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m additional_data\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'batch'"
     ]
    }
   ],
   "source": [
    "additional_data = pipeline_dt(sources, costs, slices, query_counts)\n",
    "additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224e7ea-6667-4322-97f8-fcd8a4e94150",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_x, aug_y = pipeline_augment(train_x, train_y, additional_data, train.columns)\n",
    "aug_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf5ee4-708f-45c5-8caa-80e29a8a6bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
